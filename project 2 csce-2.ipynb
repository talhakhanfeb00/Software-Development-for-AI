{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e7f73ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/talhakhan/opt/anaconda3/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in /Users/talhakhan/opt/anaconda3/lib/python3.9/site-packages (0.17.2)\n",
      "Requirement already satisfied: einops in /Users/talhakhan/opt/anaconda3/lib/python3.9/site-packages (0.8.1)\n",
      "Requirement already satisfied: numpy in /Users/talhakhan/opt/anaconda3/lib/python3.9/site-packages (1.22.4)\n",
      "Requirement already satisfied: matplotlib in /Users/talhakhan/opt/anaconda3/lib/python3.9/site-packages (3.4.3)\n",
      "Requirement already satisfied: tqdm in /Users/talhakhan/opt/anaconda3/lib/python3.9/site-packages (4.62.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/talhakhan/opt/anaconda3/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in /Users/talhakhan/opt/anaconda3/lib/python3.9/site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: filelock in /Users/talhakhan/opt/anaconda3/lib/python3.9/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: fsspec in /Users/talhakhan/opt/anaconda3/lib/python3.9/site-packages (from torch) (2021.8.1)\n",
      "Requirement already satisfied: networkx in /Users/talhakhan/opt/anaconda3/lib/python3.9/site-packages (from torch) (2.6.3)\n",
      "Requirement already satisfied: sympy in /Users/talhakhan/opt/anaconda3/lib/python3.9/site-packages (from torch) (1.9)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/talhakhan/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/talhakhan/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/talhakhan/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/talhakhan/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/talhakhan/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: six in /Users/talhakhan/opt/anaconda3/lib/python3.9/site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/talhakhan/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch) (1.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/talhakhan/opt/anaconda3/lib/python3.9/site-packages (from sympy->torch) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision einops numpy matplotlib tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c71b8da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ImageTokenizer(nn.Module):\n",
    "    def __init__(self, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)  # (B, N, D) where N is the number of patches\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42b20896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def apply_rope(x, theta=10000):\n",
    "    seq_len, dim = x.shape[-2:]\n",
    "    indices = torch.arange(seq_len, dtype=torch.float32).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, dim, 2, dtype=torch.float32) * (-torch.log(torch.tensor(theta)) / dim))\n",
    "    sinusoids = torch.sin(indices * div_term), torch.cos(indices * div_term)\n",
    "    x_sin, x_cos = torch.sin(x), torch.cos(x)\n",
    "    return x_sin * sinusoids[1] + x_cos * sinusoids[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2caaed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * mlp_ratio),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * mlp_ratio, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98d82f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b7ad289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "batch_size = 64  # Define batch size\n",
    "\n",
    "\n",
    "\n",
    "dataset = datasets.FashionMNIST(root='./Documents/data', train=True, transform=transform, download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48890e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=128, patch_size=16, in_channels=3, embed_dim=768, num_heads=12, depth=12, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.tokenizer = ImageTokenizer(patch_size, in_channels, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, (img_size // patch_size) ** 2, embed_dim))  # Positional embedding\n",
    "        self.blocks = nn.ModuleList([VisionTransformerBlock(embed_dim, num_heads) for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenizer(x) + self.pos_embed\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Define ImageTokenizer and VisionTransformerBlock from earlier code\n",
    "class ImageTokenizer(nn.Module):\n",
    "    def __init__(self, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)  # (B, N, D) where N is the number of patches\n",
    "        return x\n",
    "\n",
    "class VisionTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * mlp_ratio),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * mlp_ratio, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6700fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Adjust image size\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to 3 channels\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load FashionMNIST dataset\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ce88ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTokenizer:\n",
    "    def __init__(self, patch_size=4):\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (28 // patch_size) ** 2  # 49 patches for 28x28 images\n",
    "\n",
    "    def tokenize(self, img):\n",
    "        \"\"\"\n",
    "        Convert an image into non-overlapping patches.\n",
    "        Output shape: (batch, num_patches, patch_size^2)\n",
    "        \"\"\"\n",
    "        B, C, H, W = img.shape\n",
    "        img = rearrange(img, \"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1=self.patch_size, p2=self.patch_size)\n",
    "        return img\n",
    "\n",
    "tokenizer = ImageTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d9a4357",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.theta = 10000  # Scaling factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len, batch, dim = x.shape\n",
    "        assert dim % 2 == 0, \"Embedding dimension must be even for RoPE\"\n",
    "\n",
    "        pos = torch.arange(seq_len, device=x.device).float()\n",
    "        theta = self.theta ** (-2 * (torch.arange(dim // 2).float() / dim))\n",
    "\n",
    "        # Compute angles\n",
    "        pos_theta = torch.einsum(\"n,d->nd\", pos, theta)  # (seq_len, dim//2)\n",
    "        sin_pos = torch.sin(pos_theta)\n",
    "        cos_pos = torch.cos(pos_theta)\n",
    "\n",
    "        # Split embeddings into even and odd indices\n",
    "        x1, x2 = x[..., 0::2], x[..., 1::2]\n",
    "\n",
    "        # Apply rotation\n",
    "        x_rope = torch.cat([x1 * cos_pos - x2 * sin_pos, x1 * sin_pos + x2 * cos_pos], dim=-1)\n",
    "        return x_rope\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bab10f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionGPT(nn.Module):\n",
    "    def __init__(self, num_patches=49, embed_dim=128, num_classes=20, num_heads=4, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(16, embed_dim)  # Patch embedding (for 4x4 patches)\n",
    "        self.rope = RoPE(embed_dim)  # RoPE for positional encoding\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Convert patches to embeddings\n",
    "        x = self.rope(x)  # Apply RoPE\n",
    "        x = self.transformer(x)  # Transformer Encoder\n",
    "        x = x.mean(dim=1)  # Aggregate over patches\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd997ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/talhakhan/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model\n",
    "model = VisionGPT().to(device)  # Ensure VisionGPT is defined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b937f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddc9a2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image tokens shape: torch.Size([32, 1024, 48])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    images = tokenizer.tokenize(images)  # Convert images into tokens\n",
    "    print(\"Image tokens shape:\", images.shape)  # Debugging step\n",
    "    break  # Check only the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3600fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches: 49\n"
     ]
    }
   ],
   "source": [
    "image_size = 28  # FashionMNIST and NIST images are 28x28\n",
    "patch_size = 4   # Each patch is 4x4\n",
    "\n",
    "num_patches = (image_size // patch_size) ** 2  # 49 patches for 28x28 images\n",
    "print(f\"Number of patches: {num_patches}\")  # Should print 49\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff639a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128  # Define the embedding dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b54b825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 49, 128)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(batch_size, num_patches, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e204f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionGPT(nn.Module):\n",
    "    def __init__(self, num_patches=49, patch_dim=16, embed_dim=128, num_classes=10, num_heads=4, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(patch_dim, embed_dim)  # Ensure patch_dim matches tokenizer output\n",
    "        self.rope = RoPE(embed_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Convert patches to embeddings\n",
    "        x = self.rope(x)  # Apply RoPE\n",
    "        x = self.transformer(x)  # Transformer Encoder\n",
    "        x = x.mean(dim=1)  # Aggregate over patches\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "925fbaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f389e5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VisionGPT(nn.Module):\n",
    "    def __init__(self, image_size=28, patch_size=4, embed_dim=128, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Patch size is 4x4, so patch_dim is 16\n",
    "        patch_dim = patch_size * patch_size  # 4 * 4 = 16\n",
    "        \n",
    "        # Embedding layer: convert each patch into an embedding of size embed_dim\n",
    "        self.embedding = nn.Linear(patch_dim, embed_dim)\n",
    "        \n",
    "        # A transformer layer or any further processing can be added\n",
    "        self.transformer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=4)\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)  # num_classes = 10 for Fashion MNIST\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input image to (batch_size, num_patches, patch_dim)\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1)  # Flatten the image\n",
    "        \n",
    "        # Pass through the embedding layer\n",
    "        x = self.embedding(x)  # Convert patches to embeddings\n",
    "        \n",
    "        # Apply transformer or other layers\n",
    "        x = self.transformer(x)  # Transformer processing (if needed)\n",
    "        \n",
    "        # Take mean across patches (if using patch-based processing)\n",
    "        x = x.mean(dim=1)\n",
    "        \n",
    "        # Final classification\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model for Fashion MNIST\n",
    "model = VisionGPT(image_size=28, patch_size=4, embed_dim=128, num_classes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4573f79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.8003367146195125\n",
      "Epoch 2/5, Loss: 0.5228332801541286\n",
      "Epoch 3/5, Loss: 0.47346758513626\n",
      "Epoch 4/5, Loss: 0.4455727555318428\n",
      "Epoch 5/5, Loss: 0.42688448318858135\n",
      "Test Accuracy: 83.72%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the SimpleNN model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=128, num_classes=10):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # From 784 to 128\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  # From 128 to 10 (output layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # Pass through the first layer with ReLU activation\n",
    "        x = self.fc2(x)  # Final output layer\n",
    "        return x\n",
    "\n",
    "# Set up the transformation for data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize the image data\n",
    "])\n",
    "\n",
    "# Load the Fashion MNIST dataset\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleNN(input_size=784, hidden_size=128, num_classes=10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        # Flatten the images to (batch_size, 784)\n",
    "        images = images.view(images.size(0), -1)\n",
    "\n",
    "        # Zero the gradients, perform forward pass, compute loss, backpropagate, and update weights\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Evaluate the model on test data\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # Disable gradient tracking for inference\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(images.size(0), -1)  # Flatten the images\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the predicted labels\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6e6aad09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.7992495837559832\n",
      "Epoch 2/5, Loss: 0.5217999573518981\n",
      "Epoch 3/5, Loss: 0.4730919840684069\n",
      "Epoch 4/5, Loss: 0.4464393199633942\n",
      "Epoch 5/5, Loss: 0.42714410435670475\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Transform: Convert images to tensor and normalize\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "train_dataset = datasets.FashionMNIST(root='./Documents/data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "model = SimpleNN(input_size=784, hidden_size=128, num_classes=10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        # Flatten the images to (batch_size, 784)\n",
    "        images = images.view(images.size(0), -1)\n",
    "\n",
    "        # Zero the gradients, perform forward pass, compute loss, backpropagate, and update weights\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8b0008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Sample input batch (batch_size = 64, channels = 1, height = 28, width = 28)\n",
    "batch_size = 64\n",
    "inputs = torch.randn(batch_size, 1, 28, 28)  # Example input shape: (64, 1, 28, 28)\n",
    "\n",
    "# Reshape each image in the batch to a vector of 784 elements\n",
    "inputs = inputs.view(batch_size, -1)  # Shape becomes (64, 784)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6f0acf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.8121033735684494\n",
      "Epoch 2, Loss: 0.5250133015453688\n",
      "Epoch 3, Loss: 0.47575459896183725\n",
      "Epoch 4, Loss: 0.4479793735118563\n",
      "Epoch 5, Loss: 0.42937870982931114\n",
      "Test Accuracy: 83.68%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define transformation to convert images to tensors and normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalizing the image data\n",
    "])\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the neural network model (as above)\n",
    "model = SimpleNN(input_size=784, hidden_size=128, num_classes=10)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        # Flatten the images to (batch_size, 784)\n",
    "        images = images.view(images.size(0), -1)  # Flatten the images\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Evaluation on test data\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(images.size(0), -1)  # Flatten the images\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c306ca30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
